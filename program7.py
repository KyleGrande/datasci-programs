"""
    Name: Kyle Grande
    Email: kyle.grande72@myhunter.cuny.edu
    Resources:

"""
import pickle
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LassoCV, RidgeCV

def import_data(csv_file):
    '''
    This function takes one input:
    csv_file: the name of a CSV file time series data for commodities from FRED.
    The data in the file is read into a DataFrame, and a new column, units,
    that indexes the lines of the files and represents the number of units
    (e.g. months, weeks, years). It can be generated by looping through the
    dataset or using the index as a column (e.g. df.index.to_series()).
    The resulting DataFrame is returned.
    '''
    df = pd.read_csv(csv_file)
    df['units'] = df.index.to_series()

    return df

def split_data(df, y_col_name, test_size = 0.25, random_state = 21):
    '''
    This function takes 4 input parameters:
    df: a DataFrame containing with a columns units.
    y_col_name: the name of the column of the dependent variable.
    test_size: accepts a float between 0 and 1 and represents the
    proportion of the data set to use for training. This parameter has a default value of 0.25.
    random_state: Used as a seed to the randomization. This parameter has a default value of 1870.
    Returns the data split into 4 subsets, corresponding to those returned
    by train_test_split: x_train, x_test, y_train, and y_test. where units is
    the "x" column and the input parameter, y_col_name is the "y" column.
    Note: this is function is very similar to the splitting of data into training
    and testing sets from Program 6.
    '''
    x_col = df['units']
    y_col = df[y_col_name]

    x_train, x_test, y_train, y_test = train_test_split(x_col,
                    y_col, test_size = test_size, random_state = random_state)

    return x_train, x_test, y_train, y_test

def fit_poly(xes, yes, epsilon=100, verbose=False):
    '''
    This function takes four inputs:
    xes: a DataFrame that includes the column units.
    yes: a series of the same length as xes.
    epsilon: the size of the sample. It has a default value of 100.
    verbose: when True, prints out the MSE cost for each degree tried
    (in format: f'MSE cost for deg {deg} poly model:
    {error:.3f}' for degrees 1, 2, ...,
    until the error is below epsilon, see example below). It has a default value of False.
    It returns the smallest integer degree >= 1 for which the
    model yields a MSE of less than the specified epsilon and the coefficients
    as a vector for df["units"] and df[y_col]. If it does not find a model
    with an error less than epsilon by degree 5, returns None.
    When fitting the linear regression model, the fit_intercept=False.
    Hint: see the Chapter 16 for examples of using PolynomialFeatures().
    '''

    for degrees in range(1, 6):
        poly = PolynomialFeatures(degree=degrees)
        poly_features = poly.fit_transform(xes.values)

        model = LinearRegression(fit_intercept=False)
        model.fit(poly_features, yes)

        y_pred = model.predict(poly_features)

        error = mean_squared_error(yes, y_pred)

        if verbose:
            print(f'MSE cost for deg {degrees} poly model: {error:.3f}')

        if error < epsilon:
            return degrees


    return None

def fit_model(xes, yes, poly_deg=2, reg = "lasso"):
    '''
    xes: a series of numeric values.
    yes: a series of numeric values.
    poly_deg: the degree of the polynomial features to be created. It has a default value of 2.
    reg: The type of regularization used: ridge or lasso. It has a default value of lasso.
    This function fits a model with polynomial features using
    Lasso or Ridge regression with cross validation:
    Apply PolynomialFeatures to the xes with degree equal to poly_deg.
    If reg equals ridge, use RidgeCV to instantiate and fit a model
    to the polynomial features and yes. Otherwise, use LassoCV to to instantiate and fit the model.
    Returns the model as serialized object (i.e. a pickled object).
    '''
    poly = PolynomialFeatures(degree=poly_deg)
    poly_features = poly.fit_transform(xes.values)

    if reg == "ridge":
        model = RidgeCV()
    elif reg == "lasso":
        model = LassoCV()

    model.fit(poly_features, yes)
    mod_pkl = pickle.dumps(model)

    return mod_pkl
def predict_using_trained_model(mod_pkl, poly_xes, yes):
    '''
    This function takes three inputs:
    mod_pkl: a trained model for the data, stored in pickle format.
    poly_xes: an array or DataFrame of numeric columns with no null values.
    yes: an array or DataFrame of numeric columns with no null values.
    Computes and returns the mean squared error and r2 score between the
    values predicted by the model (mod on x) and the actual values (y).
    Note that sklearn.metrics contains two functions that may be of use:
    mean_squared_error and r2_score.
    '''
    model = pickle.loads(mod_pkl)
    y_predict = model.predict(poly_xes)
    mse = mean_squared_error(yes, y_predict)
    r2_result= r2_score(yes, y_predict)
    return mse, r2_result
