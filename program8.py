"""
    Name: Kyle Grande
    Email: kyle.grande72@myhunter.cuny.edu
    Resources:
            https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html
"""

import pickle as pkl
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score



def clean_reg(reg):
    '''
    This function takes one input parameter:
    reg: a string containing the registration status of the vehicle.
    If reg is coded as passenger 'PAS' or commercial 'COM',
    return those values. Otherwise, return 'OTHER'.
    '''
    if reg in ('PAS', 'COM'):
        return reg
    return 'OTHER'

def clean_color(col):
    '''
    This function takes one input parameter:
    col: a string containing the color of the vehicle.
    Return the following for the values of col:
    'GRAY': for GY, GRAY, GREY,SILVE, SIL, SL,
    'WHITE': for WH, WHITE,
    'BLACK': for BK, BLACK, BL,
    'BLUE': for BLUE,
    'RED': for RED, RD,
    'GREEN': for GR, GREEN,
    'BROWN': for BROWN, TAN,
    Otherwise, return 'OTHER'.
    '''
    my_color_map = {
        'GY': 'GRAY', 'GRAY': 'GRAY', 'GREY': 'GRAY', 'SILVE': 'GRAY', 'SIL': 'GRAY', 'SL': 'GRAY',
        'WH': 'WHITE', 'WHITE': 'WHITE',
        'BK': 'BLACK', 'BLACK': 'BLACK', 'BL': 'BLACK',
        'BLUE': 'BLUE',
        'RED': 'RED', 'RD': 'RED',
        'GR': 'GREEN', 'GREEN': 'GREEN',
        'BROWN': 'BROWN', 'TAN': 'BROWN'
    }
    return my_color_map.get(col, 'OTHER')

def add_indicators(df,cols=['Registration', 'Color', 'State']):
    '''
    This function has two inputs and returns a DataFrame:
    df: a DataFrame that including the columns specified in cols.
    col: a list of names of columns in the DataFrameIt has a default
    value of ['Registration', 'Color', 'State'].
    Returns the DataFrame with an additional indicator columns generated by
    get_dummies for specified columns. The drop_first flag is set
    to True to drop extraneous columns.
    '''
    return pd.get_dummies(df, columns=cols, drop_first=True)

def add_excessive_flag(df, threshold=5):
    '''
    This function has two inputs and returns a DataFrame:
    df: a DataFrame that including the columns specified in cols.
    threshold: a numeric value. The default value is 5.
    Returns the DataFrame with a new column, Excessive Tickets which
    is 0 if there's less threshold number of Tickets and 1 otherwise.
    '''
    df['Excessive Tickets'] = df['Tickets'].apply(lambda x: 1 if x > threshold else 0)
    return df

def split_data(df, x_cols, y_col, test_size = 0.25, random_state = 2023):
    '''
    This function takes 5 input parameters:
    df: a DataFrame containing with a columns units.
    x_cols: a list of the names of the column of the independent variable.
    y_col: the name of the column of the dependent variable.
    test_size: accepts a float between 0 and 1 and represents the proportion of
    the data set to use for training. This parameter has a default value of 0.25.
    random_state: Used as a seed to the randomization. This parameter has a default value of 1870.
    Returns the data split into 4 subsets, corresponding to those returned by train_test_split:
    x_train, x_test, y_train, and y_test. where units is the "x" column and the input parameter,
    y_col_name is the "y" column.
    Note: this is function is very similar to the splitting of data into training and testing
    sets from Program 6 and Program 7.
    '''
    x_col = df[x_cols]
    y_col = df[y_col]
    x_train, x_test, y_train, y_test = train_test_split(x_col,
                                        y_col, test_size=test_size, random_state=random_state)
    return x_train, x_test, y_train, y_test

def fit_model(x_train, y_train, model_type='logreg'):
    '''
    This function takes four input parameters:
    x_train: the independent variable(s) for the analysis.
    y_train: the dependent variable for the analysis.
    model_type: the type of model to use. Possible values are
    'logreg', 'svm', 'nbayes', and 'rforest'.
    See below for the specified parameters for each model. The default value for this parameter is
    'logreg'.
    Fits the specifed model to the x_train and y_train data, using sklearn.
    Additional notes for each model:
    logreg: Logistic Regression: For logistic regression, use the SVM classifier to set up the
    model, sklearn.linear_model.LogisticRegression with solver solver = 'saga',
    regularization penalty='l2', and max iterations max_iter=5000 (Note that it's the
    letter L in 'l2', not a 1.).
    nbayes: Naive Bayes: use the Gaussian Naive Bayes classifier to set up the model,
    sklearn.naive_bayes.GaussianNB.
    svm: Support Vector Machine: use the SVM classifier to set up the model,
    sklearn.svm.SVC with the radial basis function kernel RBF kernel='rbf'.
    rforest: Random Forest: use the random forest classifier to set up the model,
    sklearn.ensemble.RandomForestClassifier with 100 estimators and the
    random state set to 0 (i.e. n_estimators=100, random_state=0).
    The resulting model should be returned as bytestream, using pickle.
    Hint: for more details on setting up each of these models,
    see Lecture 8 and the associated notebooks and reading.
    '''
    if model_type == 'logreg':
        model = LogisticRegression(solver='saga', penalty='l2', max_iter=5000)
    elif model_type == 'svm':
        model = SVC(kernel='rbf')
    elif model_type == 'nbayes':
        model = GaussianNB()
    elif model_type == 'rforest':
        model = RandomForestClassifier(n_estimators=100, random_state=0)

    model.fit(x_train, y_train)
    model_bytestream = pkl.dumps(model)
    return model_bytestream

def score_model(mod_pkl,xes,yes):
    '''
    This function takes three input parameters:
    mod_pkl: a object serialization of a trained model. The possible model approaches are
    logistic regression, support vector machine, naive Bayes, and random forest.
    xes: the independent variable(s) for the analysis with the same
    dimensions as which the model was trained.
    yes: the dependent variable(s) for the analysis with the same
    dimensions as which the model was trained.
    Returns the confusion matrix for the model.
    '''
    model = pkl.loads(mod_pkl)
    y_pred = model.predict(xes)
    return confusion_matrix(yes, y_pred)

def compare_models(x_test, y_test, models):
    '''
    This function has three inputs:
    x_test: a numpy array that includes rows of equal size flattened arrays,
    y_test a numpy array that takes values 0 or 1 corresponding to the rows of x_test.
    models: a list of pickled models constructed from fit_model. The
    For each of the specified models in models, calls score() function of each model
    on x_test and y_test. The function returns the index of the model with highest accuracy
    score and its accuracy score (i.e. 0 if it is the first model in the list, 1 if it is the
    second model in the list, etc). In case of ties for the best score, return the first one
    that has that value.
    '''
    scores = []
    for model in models:
        model_pkl = pkl.loads(model)
        y_pred = model_pkl.predict(x_test)
        accuracy = accuracy_score(y_test, y_pred)
        scores.append(accuracy)

    max_index = np.argmax(scores)
    return max_index, scores[max_index]
